{"cells":[{"cell_type":"markdown","metadata":{"id":"qA37W4aJmcgv"},"source":["# Shakespeare Text Generation\n","\n","Let's create Shakespearean text by training a character RNN"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"9M1gfwhXmcgz","executionInfo":{"status":"ok","timestamp":1643447350063,"user_tz":-480,"elapsed":1378,"user":{"displayName":"Adam Murphy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj859UcD_Pcry8yTAl_WjJig-Z9IB-ycFwNCz0WLFY=s64","userId":"08787610892899884155"}}},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import os\n","import time"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_L_wdPAQmcg0","executionInfo":{"status":"ok","timestamp":1643445920730,"user_tz":-480,"elapsed":22,"user":{"displayName":"Adam Murphy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj859UcD_Pcry8yTAl_WjJig-Z9IB-ycFwNCz0WLFY=s64","userId":"08787610892899884155"}},"outputId":"7a664e75-5f7d-4760-b46c-31f30f6bca81"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n","1122304/1115394 [==============================] - 0s 0us/step\n","1130496/1115394 [==============================] - 0s 0us/step\n"]}],"source":["path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pm1FY85smcg1","executionInfo":{"status":"ok","timestamp":1643445920730,"user_tz":-480,"elapsed":14,"user":{"displayName":"Adam Murphy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj859UcD_Pcry8yTAl_WjJig-Z9IB-ycFwNCz0WLFY=s64","userId":"08787610892899884155"}},"outputId":"2a4aa4f5-0ebf-404c-88f0-feab3d1e250e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Length of text: 1,115,394 characters\n"]}],"source":["# Read and decode\n","text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n","print(f'Length of text: {len(text):,} characters')"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uht4HWVKmcg2","executionInfo":{"status":"ok","timestamp":1643445920730,"user_tz":-480,"elapsed":11,"user":{"displayName":"Adam Murphy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj859UcD_Pcry8yTAl_WjJig-Z9IB-ycFwNCz0WLFY=s64","userId":"08787610892899884155"}},"outputId":"338ce598-b304-4f16-aabc-76cd57a5812e"},"outputs":[{"output_type":"stream","name":"stdout","text":["First Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You are all resolved rather to die than to famish?\n","\n","All:\n","Resolved. resolved.\n","\n","First Citizen:\n","First, you know Caius Marcius is chief enemy to the people.\n","\n"]}],"source":["# View first 250 characters\n","print(text[:250])"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WPfuOKahmcg2","executionInfo":{"status":"ok","timestamp":1643445920731,"user_tz":-480,"elapsed":10,"user":{"displayName":"Adam Murphy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj859UcD_Pcry8yTAl_WjJig-Z9IB-ycFwNCz0WLFY=s64","userId":"08787610892899884155"}},"outputId":"d154269d-d899-4181-9a6b-2aaef981e66f"},"outputs":[{"output_type":"stream","name":"stdout","text":["65 unique characters\n"]}],"source":["# The unique characters in the file\n","vocab = sorted(set(text))\n","print(f'{len(vocab)} unique characters')"]},{"cell_type":"markdown","metadata":{"id":"jGw8TT7Kmcg3"},"source":["## Preprocess Text\n","\n","First, we need to convert the strings to numbers. We can use `tf.keras.layers.StringLookup` but first need to split the text into tokens. "]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KIYgshq1mcg3","executionInfo":{"status":"ok","timestamp":1643445923806,"user_tz":-480,"elapsed":3083,"user":{"displayName":"Adam Murphy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj859UcD_Pcry8yTAl_WjJig-Z9IB-ycFwNCz0WLFY=s64","userId":"08787610892899884155"}},"outputId":"57606a12-732e-4b19-9b9a-b5a0118ec0c8"},"outputs":[{"output_type":"stream","name":"stdout","text":["<tf.RaggedTensor [[b'h', b'e', b'l', b'l', b'o'], [b'w', b'o', b'r', b'l', b'd']]>\n","<tf.RaggedTensor [[47, 44, 51, 51, 54], [62, 54, 57, 51, 43]]>\n","<tf.RaggedTensor [[b'h', b'e', b'l', b'l', b'o'], [b'w', b'o', b'r', b'l', b'd']]>\n","[b'hello' b'world']\n"]}],"source":["example_texts = ['hello', 'world']\n","chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n","print(chars)\n","\n","ids_from_chars = tf.keras.layers.StringLookup(\n","    vocabulary=list(vocab),\n","    mask_token=None\n",")\n","\n","example_ids = ids_from_chars(chars)\n","# Here we see that 'l' is mapped to 51\n","print(example_ids)\n","\n","# We need to set invert=True so that we can get human\n","# readable text back. We use get_vocabulary() to esnure\n","# [UNK] tokens are set the same way\n","chars_from_ids = tf.keras.layers.StringLookup(\n","    vocabulary=ids_from_chars.get_vocabulary(),\n","    invert=True,\n","    mask_token=None\n",")\n","\n","reproduced_chars = chars_from_ids(example_ids)\n","print(reproduced_chars)\n","reproduced_text = tf.strings.reduce_join(chars, axis=-1).numpy()\n","print(reproduced_text)\n","\n","def text_from_ids(ids):\n","    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"]},{"cell_type":"markdown","metadata":{"id":"gcI7RdDmmcg4"},"source":["We use a character RNN (rather than word) since there are far less characters to predict than possible words. Each input is a sequence of characters and the prediction is the next character.\n","\n","## Create Train Examples and Targets\n","\n","We make train sequences with `seq_length` elements. The target sequences are the same length but shifted one char to the right. So if `hello` is our text, the input is `hell` and the target sequence `ello`. \n","\n","Let's make text vector into character indicies."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ExwiYxmamcg5","executionInfo":{"status":"ok","timestamp":1643445924336,"user_tz":-480,"elapsed":536,"user":{"displayName":"Adam Murphy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj859UcD_Pcry8yTAl_WjJig-Z9IB-ycFwNCz0WLFY=s64","userId":"08787610892899884155"}},"outputId":"c574ce2c-4532-4e42-c3d5-91963cb9b6ff"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"]},"metadata":{},"execution_count":7}],"source":["all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n","all_ids"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X428es2Imcg5","executionInfo":{"status":"ok","timestamp":1643445924336,"user_tz":-480,"elapsed":23,"user":{"displayName":"Adam Murphy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj859UcD_Pcry8yTAl_WjJig-Z9IB-ycFwNCz0WLFY=s64","userId":"08787610892899884155"}},"outputId":"955d72da-67de-41fb-f1b9-6eed1f95fc14"},"outputs":[{"output_type":"stream","name":"stdout","text":["F\n","i\n","r\n","s\n","t\n"," \n","C\n","i\n","t\n","i\n"]}],"source":["ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n","# Print first elements in ids_dataset\n","for ids in ids_dataset.take(10):\n","    print(chars_from_ids(ids).numpy().decode('utf-8'))"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"FNOZSSZ3mcg6","executionInfo":{"status":"ok","timestamp":1643445924337,"user_tz":-480,"elapsed":21,"user":{"displayName":"Adam Murphy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj859UcD_Pcry8yTAl_WjJig-Z9IB-ycFwNCz0WLFY=s64","userId":"08787610892899884155"}}},"outputs":[],"source":["seq_length = 100\n","examples_per_epoch = len(text) // (seq_length+1)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oR66QV7Hmcg6","executionInfo":{"status":"ok","timestamp":1643445924337,"user_tz":-480,"elapsed":20,"user":{"displayName":"Adam Murphy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj859UcD_Pcry8yTAl_WjJig-Z9IB-ycFwNCz0WLFY=s64","userId":"08787610892899884155"}},"outputId":"c8bacc29-6d3e-429d-ee67-015e50d4b30d"},"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n"," b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n"," b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n"," b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n"," b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n"," b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n"," b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n"," b'o' b'u' b' '], shape=(101,), dtype=string)\n"]}],"source":["# Use .batch() to convert individual characters to sequences of\n","# desired size\n","sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n","# Print the first sequence (that includes input + target)\n","for seq in sequences.take(1):\n","    print(chars_from_ids(seq))"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JaNJshZ0mcg7","executionInfo":{"status":"ok","timestamp":1643445924338,"user_tz":-480,"elapsed":19,"user":{"displayName":"Adam Murphy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj859UcD_Pcry8yTAl_WjJig-Z9IB-ycFwNCz0WLFY=s64","userId":"08787610892899884155"}},"outputId":"d7b78041-39c2-43fc-914c-767ded938d04"},"outputs":[{"output_type":"stream","name":"stdout","text":["b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n","b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n","b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n","b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n","b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"]}],"source":["# Easier to see if join tokens back into strings\n","for seq in sequences.take(5):\n","    print(text_from_ids(seq).numpy())"]},{"cell_type":"markdown","metadata":{"id":"UKwnj0unmcg7"},"source":["Not sure why it just includes the sequence once and then moves on to the next sentence, seems like we could massively increase our training data by giving it more samples that way.\n","\n","Also not sure why the target is also a sequence of the exact same length, why is it not a sequence with one character?"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"M9G_RtJmmcg7","executionInfo":{"status":"ok","timestamp":1643445924338,"user_tz":-480,"elapsed":17,"user":{"displayName":"Adam Murphy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj859UcD_Pcry8yTAl_WjJig-Z9IB-ycFwNCz0WLFY=s64","userId":"08787610892899884155"}}},"outputs":[],"source":["def split_input_target(sequence):\n","    input_text = sequence[:-1]\n","    target_text = sequence[1:]\n","    return input_text, target_text"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L-YNRFhZmcg8","executionInfo":{"status":"ok","timestamp":1643445924339,"user_tz":-480,"elapsed":18,"user":{"displayName":"Adam Murphy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj859UcD_Pcry8yTAl_WjJig-Z9IB-ycFwNCz0WLFY=s64","userId":"08787610892899884155"}},"outputId":"5b31a402-d91b-4873-a229-e8dc1d5660c7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(['H', 'e', 'l', 'l', 'o', ' ', 'W', 'o', 'r', 'l', 'd'],\n"," ['e', 'l', 'l', 'o', ' ', 'W', 'o', 'r', 'l', 'd', '!'])"]},"metadata":{},"execution_count":13}],"source":["split_input_target(list('Hello World!'))"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"VJ7cOJT7mcg8","executionInfo":{"status":"ok","timestamp":1643445924339,"user_tz":-480,"elapsed":15,"user":{"displayName":"Adam Murphy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj859UcD_Pcry8yTAl_WjJig-Z9IB-ycFwNCz0WLFY=s64","userId":"08787610892899884155"}}},"outputs":[],"source":["dataset = sequences.map(split_input_target)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3gWX7crRmcg8","executionInfo":{"status":"ok","timestamp":1643445924340,"user_tz":-480,"elapsed":16,"user":{"displayName":"Adam Murphy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj859UcD_Pcry8yTAl_WjJig-Z9IB-ycFwNCz0WLFY=s64","userId":"08787610892899884155"}},"outputId":"a5a3cbdf-2a6c-4874-b2f1-9e6cf4bfd731"},"outputs":[{"output_type":"stream","name":"stdout","text":["Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n","Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n","\n","Input : b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you '\n","Target: b're all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n","\n","Input : b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us k\"\n","Target: b\"ow Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n","\n"]}],"source":["for input_example, target_example in dataset.take(3):\n","    print('Input :', text_from_ids(input_example).numpy())\n","    print('Target:', text_from_ids(target_example).numpy())\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"X37FYlI7mcg9"},"source":["Not sure why they are treating this like a character RNN... I guess the point is that it is predicting characters and not words. But we are grouping loads of characters together in both the input and target and that way it will learn how to use English.\n","\n","Note also that we feed the data in as a sequence into an LSTM layer that is designed to work explicity with sequences. Since the input and output sequences are the same length, each input char gets mapped to the next one. We don't have to manually create the dataset like 100 input and 1 output but if we have 100 input and 100 output, we get 100 steps of learning when we feed each sequence in.\n","\n","## Create Training Batches\n","\n","Now we need to shuffle and pack the data into batches. "]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cgl013Swmcg9","executionInfo":{"status":"ok","timestamp":1643445924340,"user_tz":-480,"elapsed":14,"user":{"displayName":"Adam Murphy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj859UcD_Pcry8yTAl_WjJig-Z9IB-ycFwNCz0WLFY=s64","userId":"08787610892899884155"}},"outputId":"c0a2815f-2bad-48ab-e63c-463e068a3096"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"]},"metadata":{},"execution_count":16}],"source":["BATCH_SIZE = 64\n","\n","# TF Data is designed to work with infinite seqs, so doesn't shuffle\n","# entire seq in memory. Instead, it has a buffer in which it shuffles\n","# elements\n","BUFFER_SIZE = 10000\n","\n","dataset = (\n","    dataset\n","    .shuffle(BUFFER_SIZE)\n","    .batch(BATCH_SIZE, drop_remainder=True)\n","    .prefetch(tf.data.experimental.AUTOTUNE)\n",")\n","\n","dataset"]},{"cell_type":"markdown","metadata":{"id":"Mxj20hlVmcg9"},"source":["Note that even though it's easier to work with just characters, we still apply an embedding so that the chars can learn some relationships to each other. \n","\n","## Let's Build the Model"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"KtS2m3A_mcg9","executionInfo":{"status":"ok","timestamp":1643445924340,"user_tz":-480,"elapsed":11,"user":{"displayName":"Adam Murphy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj859UcD_Pcry8yTAl_WjJig-Z9IB-ycFwNCz0WLFY=s64","userId":"08787610892899884155"}}},"outputs":[],"source":["from tensorflow.keras.layers import Embedding, GRU, Dense"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"83ZN_L7cmcg-","executionInfo":{"status":"ok","timestamp":1643445924341,"user_tz":-480,"elapsed":12,"user":{"displayName":"Adam Murphy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj859UcD_Pcry8yTAl_WjJig-Z9IB-ycFwNCz0WLFY=s64","userId":"08787610892899884155"}}},"outputs":[],"source":["vocab_size = len(vocab)\n","embedding_dim = 256\n","rnn_units = 1024\n","\n","class MyModel(tf.keras.Model):\n","    \n","    def __init__(self, vocab_size, embedding_dim, rnn_units):\n","        super().__init__(self)\n","        self.embedding = Embedding(vocab_size, embedding_dim)\n","        self.gru = GRU(rnn_units, return_sequences=True, return_state=True)\n","        self.dense = Dense(vocab_size)\n","\n","    def call(self, inputs, states=None, return_state=False, training=False):\n","        x = inputs\n","        x = self.embedding(x, training=training)\n","        if states is None:\n","            states = self.gru.get_initial_state(x)\n","        x, states = self.gru(x, initial_state=states, training=training)\n","        x = self.dense(x, training=training)\n","\n","        if return_state:\n","            return x, states\n","        else:\n","            return x"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"Z7dGTSWMmcg-","executionInfo":{"status":"ok","timestamp":1643445924341,"user_tz":-480,"elapsed":12,"user":{"displayName":"Adam Murphy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj859UcD_Pcry8yTAl_WjJig-Z9IB-ycFwNCz0WLFY=s64","userId":"08787610892899884155"}}},"outputs":[],"source":["model = MyModel(\n","    # Make sure vocab size matches the StringLookup layer\n","    vocab_size=len(ids_from_chars.get_vocabulary()),\n","    embedding_dim=embedding_dim,\n","    rnn_units=rnn_units\n",")"]},{"cell_type":"markdown","metadata":{"id":"IqbbO7NKmcg-"},"source":["## Test Model"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6dnQFDsTmcg-","executionInfo":{"status":"ok","timestamp":1643445934936,"user_tz":-480,"elapsed":10606,"user":{"displayName":"Adam Murphy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj859UcD_Pcry8yTAl_WjJig-Z9IB-ycFwNCz0WLFY=s64","userId":"08787610892899884155"}},"outputId":"b6ced577-2926-4493-b789-fa4e6c1b47d0"},"outputs":[{"output_type":"stream","name":"stdout","text":["(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"]}],"source":["for input_example_batch, target_example_batch in dataset.take(1):\n","    example_batch_predictions = model(input_example_batch)\n","    print(example_batch_predictions.shape, '# (batch_size, sequence_length, vocab_size)')"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NWyJghlamcg-","executionInfo":{"status":"ok","timestamp":1643445934937,"user_tz":-480,"elapsed":15,"user":{"displayName":"Adam Murphy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj859UcD_Pcry8yTAl_WjJig-Z9IB-ycFwNCz0WLFY=s64","userId":"08787610892899884155"}},"outputId":"07e0cf9d-38df-478d-be7d-fcceec4d8899"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"my_model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       multiple                  16896     \n","                                                                 \n"," gru (GRU)                   multiple                  3938304   \n","                                                                 \n"," dense (Dense)               multiple                  67650     \n","                                                                 \n","=================================================================\n","Total params: 4,022,850\n","Trainable params: 4,022,850\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["# We lose access to model.summary() output shape when we subclass model\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"uZG6UnKAmcg_"},"source":["**IMPORTANT** to get the actual predictions from the model you must *sample from the output distribution* to get the char indicies. If you take argmax, the model can get stuck in an infinite loop. "]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0MMUs7Bkmcg_","executionInfo":{"status":"ok","timestamp":1643445934938,"user_tz":-480,"elapsed":13,"user":{"displayName":"Adam Murphy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj859UcD_Pcry8yTAl_WjJig-Z9IB-ycFwNCz0WLFY=s64","userId":"08787610892899884155"}},"outputId":"30d5747f-39df-4367-a9b6-24b71bc377d1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([28, 20, 64, 27, 53, 12, 25, 16,  4,  7,  8, 20, 31, 30, 62, 46,  1,\n","       63, 44, 44, 23, 23, 65, 10, 46, 24, 56, 40,  3, 39,  6, 13, 34, 32,\n","       28, 34, 10,  2, 41, 35, 58,  7, 15, 23, 61, 46, 32, 33,  8, 20, 32,\n","       60, 56, 31, 53,  6,  4, 28, 54, 58, 61, 53, 63, 40, 59, 21, 41, 14,\n","       42, 32, 34, 38, 27, 65, 15, 38, 29, 31, 34, 34, 23, 41, 50, 11, 54,\n","       32, 14,  1, 25, 50,  2,  2, 64,  0,  4, 57, 55, 44, 54, 51])"]},"metadata":{},"execution_count":22}],"source":["sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n","sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n","sampled_indices"]},{"cell_type":"markdown","metadata":{"id":"lzyoFXPymcg_"},"source":["This is a bit weird since we will get different values each time we run it. However, we pass in the logits (`example_batch_predictions`) which represent the probability of each character. The `tf.random.categorical` function is sampling from that distribution so those values with a higher probability will be chosen much more often. I guess randomness is good since there is not always one character that must follow the next. \n","\n","Note also that this model is currently untrained and so is not going to produce anything coherent. "]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YE4U5-momcg_","executionInfo":{"status":"ok","timestamp":1643445934939,"user_tz":-480,"elapsed":13,"user":{"displayName":"Adam Murphy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj859UcD_Pcry8yTAl_WjJig-Z9IB-ycFwNCz0WLFY=s64","userId":"08787610892899884155"}},"outputId":"9d537cae-464f-4fd6-d40e-48d17eb0083a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Input:\n"," b\"you do me wrong.\\n\\nROMEO:\\nTut, I have lost myself; I am not here;\\nThis is not Romeo, he's some other \"\n","\n","Next char predictions:\n"," b\"OGyNn;LC$,-GRQwg\\nxeeJJz3gKqa!Z'?USOU3 bVs,BJvgST-GSuqRn'$OosvnxatHbAcSUYNzBYPRUUJbk:oSA\\nLk  y[UNK]$rpeol\"\n"]}],"source":["print('Input:\\n', text_from_ids(input_example_batch[0]).numpy())\n","print()\n","print('Next char predictions:\\n', text_from_ids(sampled_indices).numpy())"]},{"cell_type":"markdown","metadata":{"id":"iIYpK4Ulmcg_"},"source":["# Time to Train\n","\n","Now we just have a classification problem where each class is a character."]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nUze7bgXmchA","executionInfo":{"status":"ok","timestamp":1643445934939,"user_tz":-480,"elapsed":12,"user":{"displayName":"Adam Murphy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj859UcD_Pcry8yTAl_WjJig-Z9IB-ycFwNCz0WLFY=s64","userId":"08787610892899884155"}},"outputId":"d7dbdd88-3d68-4ab4-8986-308c79d1b5fe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Pred shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n","Mean loss:   tf.Tensor(4.1897755, shape=(), dtype=float32)\n"]}],"source":["loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n","\n","example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n","print('Pred shape: ', example_batch_predictions.shape, ' # (batch_size, sequence_length, vocab_size)')\n","print('Mean loss:  ', example_batch_mean_loss)"]},{"cell_type":"markdown","metadata":{"id":"J1jmxjD8mchA"},"source":["We expect an untrained model to be unsure of itself. So, the output of all the logits should be similar magnitude. We can check this is true by comparing the exp of the mean loss to the vocab size. "]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DHGPDGedmchA","executionInfo":{"status":"ok","timestamp":1643445934940,"user_tz":-480,"elapsed":12,"user":{"displayName":"Adam Murphy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj859UcD_Pcry8yTAl_WjJig-Z9IB-ycFwNCz0WLFY=s64","userId":"08787610892899884155"}},"outputId":"40ca8011-e456-41da-fa23-31af40f61850"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(66.007965, 65)"]},"metadata":{},"execution_count":25}],"source":["tf.exp(example_batch_mean_loss).numpy(), vocab_size"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"-mNAKIAQmchA","executionInfo":{"status":"ok","timestamp":1643445934940,"user_tz":-480,"elapsed":9,"user":{"displayName":"Adam Murphy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj859UcD_Pcry8yTAl_WjJig-Z9IB-ycFwNCz0WLFY=s64","userId":"08787610892899884155"}}},"outputs":[],"source":["model.compile(optimizer='adam', loss=loss)\n","\n","checkpoint_dir = './text_gen_train_checkpoints'\n","# Name of checkpoint files\n","checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n","\n","checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_prefix,\n","    save_weights_only=True\n",")"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EHtSbklgmchA","executionInfo":{"status":"ok","timestamp":1643446625780,"user_tz":-480,"elapsed":211694,"user":{"displayName":"Adam Murphy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj859UcD_Pcry8yTAl_WjJig-Z9IB-ycFwNCz0WLFY=s64","userId":"08787610892899884155"}},"outputId":"b7ef3580-cf2e-4bb6-eedf-891535559e06"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","172/172 [==============================] - 34s 137ms/step - loss: 2.7111\n","Epoch 2/20\n","172/172 [==============================] - 25s 128ms/step - loss: 1.9869\n","Epoch 3/20\n","172/172 [==============================] - 23s 126ms/step - loss: 1.7099\n","Epoch 4/20\n","172/172 [==============================] - 24s 129ms/step - loss: 1.5496\n","Epoch 5/20\n","172/172 [==============================] - 23s 128ms/step - loss: 1.4514\n","Epoch 6/20\n","172/172 [==============================] - 23s 127ms/step - loss: 1.3825\n","Epoch 7/20\n","172/172 [==============================] - 24s 127ms/step - loss: 1.3297\n","Epoch 8/20\n","172/172 [==============================] - 23s 127ms/step - loss: 1.2854\n","Epoch 9/20\n","172/172 [==============================] - 23s 127ms/step - loss: 1.2436\n","Epoch 10/20\n","172/172 [==============================] - 24s 131ms/step - loss: 1.2046\n","Epoch 11/20\n","172/172 [==============================] - 27s 134ms/step - loss: 1.1648\n","Epoch 12/20\n","172/172 [==============================] - 25s 131ms/step - loss: 1.1228\n","Epoch 13/20\n","172/172 [==============================] - 26s 132ms/step - loss: 1.0807\n","Epoch 14/20\n","172/172 [==============================] - 24s 130ms/step - loss: 1.0350\n","Epoch 15/20\n","172/172 [==============================] - 23s 127ms/step - loss: 0.9866\n","Epoch 16/20\n","172/172 [==============================] - 24s 128ms/step - loss: 0.9353\n","Epoch 17/20\n","172/172 [==============================] - 23s 126ms/step - loss: 0.8825\n","Epoch 18/20\n","172/172 [==============================] - 23s 128ms/step - loss: 0.8308\n","Epoch 19/20\n","172/172 [==============================] - 23s 127ms/step - loss: 0.7789\n","Epoch 20/20\n","172/172 [==============================] - 23s 127ms/step - loss: 0.7301\n"]}],"source":["EPOCHS = 20\n","history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"]},{"cell_type":"markdown","source":["## Generate Text\n","\n","To generate text, run the model in a loop, and keep track of the internal state as you execute. \n","\n","Each time you run the model, pass in some text + internal state. The model returns next char prediction and the new state. Pass the prediction + state back in and you will continue generating text. \n","\n","Let's make a single step prediction. "],"metadata":{"id":"cEdVZcWUmwOj"}},{"cell_type":"code","execution_count":28,"metadata":{"id":"gN-25ZnBmchB","executionInfo":{"status":"ok","timestamp":1643446625781,"user_tz":-480,"elapsed":3,"user":{"displayName":"Adam Murphy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj859UcD_Pcry8yTAl_WjJig-Z9IB-ycFwNCz0WLFY=s64","userId":"08787610892899884155"}}},"outputs":[],"source":["class OneStep(tf.keras.Model):\n","    \n","    def __init__(\n","        self, \n","        model, \n","        chars_from_ids, \n","        ids_from_chars, \n","        temperature=1.0\n","        ):\n","        super().__init__()\n","        self.temperature = temperature\n","        self.model = model\n","        self.chars_from_ids = chars_from_ids\n","        self.ids_from_chars = ids_from_chars\n","\n","        # Create mask to avoid '[UNK]' being predicted\n","        skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n","        sparse_mask = tf.SparseTensor(\n","            # Put -inf at each bad index\n","            values = [-float('inf')] * len(skip_ids),\n","            indices=skip_ids,\n","            # Match shape to vocab\n","            dense_shape=[len(ids_from_chars.get_vocabulary())]\n","        )\n","        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n","\n","    @tf.function\n","    def generate_one_step(self, inputs, states=None):\n","        # Convert strings to token IDs\n","        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n","        input_ids = self.ids_from_chars(input_chars).to_tensor()\n","\n","        # Run model\n","        # predicted_logits.shape is [batch, char, next_char_logits]\n","        predicted_logits, states = self.model(inputs=input_ids, \n","                                                states=states,\n","                                                return_state=True)\n","        \n","        # Only use last prediction\n","        predicted_logits = predicted_logits[:, -1, :]\n","        # Control randomness of predictions\n","        predicted_logits = predicted_logits / self.temperature\n","        # Apply prediction mask and prevent '[UNK]' from being generated\n","        # Having a logit of -inf means this will never be predicted\n","        predicted_logits = predicted_logits + self.prediction_mask\n","\n","        # Sample output logits to generate predicted token IDs\n","        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n","        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n","\n","        # Convert from token ids to chars\n","        predicted_chars = self.chars_from_ids(predicted_ids)\n","\n","        # Return chars and model state\n","        return predicted_chars, states\n"]},{"cell_type":"code","source":["one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"],"metadata":{"id":"7heL0fq4puem","executionInfo":{"status":"ok","timestamp":1643446625781,"user_tz":-480,"elapsed":2,"user":{"displayName":"Adam Murphy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj859UcD_Pcry8yTAl_WjJig-Z9IB-ycFwNCz0WLFY=s64","userId":"08787610892899884155"}}},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":["Now just run this model in a loop to generate text!! Very cool!\n","\n","Model already knows about capitalization, can make paragraphs and can also imitate Shakespeare-esque writing style. But (due mainly to small number of training epochs) it cannot form coherent sentences."],"metadata":{"id":"vGjmFpKJp5Op"}},{"cell_type":"code","source":["start = time.time()\n","states = None\n","next_char = tf.constant(['LADY MACBETH:'])\n","result = [next_char]\n","\n","for n in range(1000):\n","    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n","    result.append(next_char)\n","    if n < 7:\n","        print(f'Result after {n}-th iteration')\n","        print(tf.strings.join(result)[0].numpy().decode('utf-8'))\n","        print()\n","        result_7 = tf.strings.join(result)\n","\n","result = tf.strings.join(result)\n","end = time.time()\n","print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n","print(f'\\nRun time: {end - start:.2f}s')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ot0ifV6up38Y","executionInfo":{"status":"ok","timestamp":1643446630369,"user_tz":-480,"elapsed":4590,"user":{"displayName":"Adam Murphy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj859UcD_Pcry8yTAl_WjJig-Z9IB-ycFwNCz0WLFY=s64","userId":"08787610892899884155"}},"outputId":"da4f1dc5-939a-4cc4-e755-89782c636515"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Result after 0-th iteration\n","LADY MACBETH:\n","\n","\n","Result after 1-th iteration\n","LADY MACBETH:\n","B\n","\n","Result after 2-th iteration\n","LADY MACBETH:\n","Be\n","\n","Result after 3-th iteration\n","LADY MACBETH:\n","Be \n","\n","Result after 4-th iteration\n","LADY MACBETH:\n","Be h\n","\n","Result after 5-th iteration\n","LADY MACBETH:\n","Be he\n","\n","Result after 6-th iteration\n","LADY MACBETH:\n","Be he \n","\n","LADY MACBETH:\n","Be he the tyrann, ay, that all the banns thou canst grow you our ten this\n","there? the Earl of Wrench, that now is of\n","make othereign and to pity; it hath born to me;\n","Near yet some greeting to his disight, to flatter'd then\n","for sheep, ridges of golden cross, and in prison with the tape.\n","\n","Pedant:\n","O maid, my lord; 'tis your sword like one set down.\n","\n","POMPEY:\n","Three, poor hearing and conquest of it! And to the cord\n","Would lay in thee, poison, elecoments,\n","My feating work, I, and myself are wears,\n","Are for whose cheeks in death.\n","\n","PAULINA:\n","Not so,\n","And therefore I'll become you measure mercy.\n","\n","QUEEN ELIZABETH:\n","Would nother, grow together, Kate: but green!\n","\n","All:\n","Long live King Henry.\n","\n","CLIFFORD:\n","O Thom I come, my lord.\n","\n","LEONTES:\n","Trave, 'twas rather kiss my life\n","And many answers fresh from at the charm,\n","I'll follow proclaim'd: and there's no realmen'd hat\n","For Norfolk, when indeed she be as for you:\n","'Tis in no more successfulty;\n","O'er laschise or foul and all, as he does make,\n","Will marry her own lives m \n","\n","________________________________________________________________________________\n","\n","Run time: 4.43s\n"]}]},{"cell_type":"code","source":["result_7"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LnDvv4Xmrx2K","executionInfo":{"status":"ok","timestamp":1643446630370,"user_tz":-480,"elapsed":5,"user":{"displayName":"Adam Murphy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj859UcD_Pcry8yTAl_WjJig-Z9IB-ycFwNCz0WLFY=s64","userId":"08787610892899884155"}},"outputId":"4abbacc1-208e-4bf0-a1de-dbffb29fc695"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'LADY MACBETH:\\nBe he '], dtype=object)>"]},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","source":["**IMPROVE PERFORMANCE**\n","Simplest way to improve the model is to train longer. Then can add in more layers and also play with `temperature` to see how random the predictions are.\n","\n","**IMPROVE TEXT GENERATION SPEED**\n","Simplest way is to batch it. In other words, give it more than one starting character. It takes the same amount of time to produce 5 outputs as it does to create 1. "],"metadata":{"id":"K9JxI4D2Ay7E"}},{"cell_type":"code","source":["start = time.time()\n","states = None\n","# Pass in multiple inputs\n","next_char = tf.constant(['LADY MACBETH:', \n","                         'KING LEAR:', \n","                         'Will you marry me?', \n","                         'HAMLET:', \n","                         'To be or not to be'])\n","result = [next_char]\n","\n","for n in range(1000):\n","    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n","    result.append(next_char)\n","\n","result = tf.strings.join(result)\n","end = time.time()\n","# Just print out result for ease\n","print(result, '\\n\\n' + '_'*80)\n","print(f'\\nRun time: {end - start:.2f}s')"],"metadata":{"id":"GusGbUrgt1gv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643447382421,"user_tz":-480,"elapsed":8464,"user":{"displayName":"Adam Murphy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj859UcD_Pcry8yTAl_WjJig-Z9IB-ycFwNCz0WLFY=s64","userId":"08787610892899884155"}},"outputId":"19ceeec3-233c-474e-fe79-5325906c13a2"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[b\"LADY MACBETH:-vie; this news before the prisoner\\nstorm to make my power true service burns to help to tarn far\\nissued.\\n\\nGRUMIO:\\nAy, my notime Edward for myself: some touch poor grants\\nWhich cannot choose but help thereof; friar.\\n\\nBAPTISTA:\\nI am content.\\nHis train-learness part these he now,\\nBy thus twenty thousand crows'd: besides a cess\\nThat thought to see their frowns made balladempt\\nTo reconcile thee against God was burn'd\\nYour trust my Richard mark'd by thy death.\\n\\nCATESBY:\\nI'll slay the people against their stomach;\\nAnd with a piech of small begin poise doth eye\\nSince his true king's falch, here since we doth great\\nHis throal of impartius.\\n\\nCOMINIUS:\\nTush!\\nI will learn to fight;\\nAnd all the master is the wind when take\\nThe wind shall make an one attend on Padua\\ncan stir a little burnt in arms: shall we go forth\\nAs monarding thus and warlike serviced,\\nSo safely in large mazed; for Polixenes\\nIm presentations of return, from wind,\\nSleeping and scatter'd battle Lucentix\\nTo see if interchangh more \"\n"," b\"KING LEAR:! Play 'em Bannardo;\\nSo doves not longer have enemies.\\n\\nSecond Murderer:\\nIn one risuse of England's king is wonderfuit;\\nAnd what you are tomanged to't, that's not all his Mates?\\n\\nWas ever well known the posterns.\\nThey are inclined to have his father is\\ncogning how, or wite, one hate cruel the dung.\\n\\nHENRY BOLINGBROKE:\\nThen, pardon me, indeed.\\nThen, good prince, these youthful lord!\\n\\nLADY CAPULET:\\nWell, good forbear; therefore for his mind of Clarence,\\nnot in both gapper to any man.\\n\\nSAMPSON:\\nIf to do see it is too face to fail in that the end.\\n\\nGRUMIO:\\nAmen, amen!\\n\\nBUCKINGHAM:\\nDorse the queen of Catesby\\nPorte all this one of theirself, thyself excuses,\\nNor fought waves here the loss of thee arm'd:\\nLet him not awak. A wise a maid.\\n\\nKATHARINA:\\nWhat then?\\n\\nFirst Servingman:\\nWhat, brother?\\n\\nKING RICHARD III:\\nGood morrow, not a man, Pointines.\\n\\nROMEO:\\nThough the caparrined how to comes, who was a bawd!\\nCome, I holp out of his that know stoops there lies\\nThe nobles henceforth shake your good\"\n"," b\"Will you marry me?\\n\\nGLOUCESTER:\\nHechard, now I beseech you, fellow,\\nBoth there the gulf itself among my brother\\nAbout a parlous bot of all.\\n\\nLUCENTIO:\\nAre you then born.\\n\\nVOLUMNIA:\\nNow, in him eye our fire; or, if Angelo\\nIs not my tears are fairly Whose course from such descent,\\nWith a poor full soon or other less\\nThan I might hand the palace; or, are\\nAn itsure of but defacer,\\nIn speads corns to die, I fear, I fear,\\nThat smils he'll to divine, that knows not\\nLoved he hath not.\\n\\nSICINIUS:\\nHave you done with me?\\n\\nBRUTUS:\\nLet's forfell this from my soul,\\nWho profort in blood, being a man,\\nWhen he makes out to hold night shall eyes opperiga,\\nBut with a power hepp of thy foot and suffer,\\nLike to a three-pyind sinis of his own soul's ingrt\\nWeathingly: go with me.\\n\\nGRUMIO:\\nI thought love him. God shall be thus, and all Hopeo's name: I shall.\\nHow say you? you must obtend his father?\\n\\nCLARENCE:\\nAy, and many man, hear, grace born! I think, one word.\\n\\nQUEEN MARGARET:\\nGo on, they would not bless thee? stay with a c\"\n"," b\"HAMLET:'s your did;\\nAnd yet so fall unto my fidecess.\\n\\nQUEEN MARGARET:\\nThanks, grave shall all ready, that was his son a king,\\nAre stonging and salt unto the grassmony's gay,\\nEither heart what banish'd mew again:\\nThere lies humility, methinks, you do.\\n\\nWARWICK:\\nThen break his course is long to deniar she stoop:\\nBy these known to the oasing thing more mild,\\nAnd then have done confessing kiss'd\\nBy, lords; for thou art to comfort; and, for the petty\\nWhen forget men that beard their basings;\\nAfter I seet it odd: away, away!\\n\\nDUKE VINCENTIO:\\nConvenient is the throne to give me your state,\\nTo get him coming hither night.\\n\\nBRUTUS:\\nGo on, Canother,\\nEaring thee fetch the present public,\\nAnd they urge to't--taining discourse of ours.\\n\\nROMEO:\\nBy and charge, and do thee so! Bay love,\\nI'll join-wife, for work, I protest! Thou dost it luck\\nto bown; but clean to--\\nA flattering grieve to Romeo!\\nDear I, that I will tell him of all their suits\\nArd knows of thatque on his cause, in safety\\nAs from your settled f\"\n"," b\"To be or not to be a blot, from fast,\\nWhat doth beholder all the rest; For Northumber\\nPlough the taste, or an another hate\\nOr clot enough now, that robbers him a senate\\nAffrights him 'Become to flank again, every foot,\\nyour abmony eyes; for 'tis his behaviors are in: then\\nevery where you do whip your leave Katharina yourself\\nDid not then but smotht hither lustful Edward.\\nDie, signior late: let us a subject, mark you.\\n\\nTRANIO:\\nSo this is another Grum of great Bornapice,\\nTo see fair hate, my choler 'scapes.\\n\\nPROSPERO:\\nAnd Salishbusy;\\nLet mightst him Girgh, and long to see me intells;\\nAnd with interp'dat to find out of their late to give;\\nShe is not dally doth make loar inrution speak,\\nWere best to me that perish'd the water should.\\n\\nGONZALO:\\nIt is a wonder, sir.\\n\\nGREMIO:\\nBut what said you? you have wastedly glad to yours.\\n\\nKING RICHARD III:\\nExton, do you not; more come interrops of sons,\\nLet them affright upon his dressing tempest:\\nSuch as foolisher, as I live, so braved,\\nI'ld with turning witest from Alo\"], shape=(5,), dtype=string) \n","\n","________________________________________________________________________________\n","\n","Run time: 7.88s\n"]}]},{"cell_type":"markdown","source":["## Export Generator\n","\n"],"metadata":{"id":"s6o5YnikGGpj"}},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ES-9jHMIGz8G","executionInfo":{"status":"ok","timestamp":1643447548059,"user_tz":-480,"elapsed":690,"user":{"displayName":"Adam Murphy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj859UcD_Pcry8yTAl_WjJig-Z9IB-ycFwNCz0WLFY=s64","userId":"08787610892899884155"}},"outputId":"d3184ec2-9714-4455-f34e-d1b035451d99"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["one_step_text_gen_model  sample_data  text_gen_train_checkpoints\n"]}]},{"cell_type":"code","source":["tf.saved_model.save(one_step_model, 'one_step_text_gen_model')\n","one_step_reloaded = tf.saved_model.load('one_step_text_gen_model')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pYCV8JhJGe83","executionInfo":{"status":"ok","timestamp":1643447598575,"user_tz":-480,"elapsed":18714,"user":{"displayName":"Adam Murphy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj859UcD_Pcry8yTAl_WjJig-Z9IB-ycFwNCz0WLFY=s64","userId":"08787610892899884155"}},"outputId":"1261c2e8-0868-4304-aac4-c8f2d362a203"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7fd7bbfd6b10>, because it is not built.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7fd7bbfd6b10>, because it is not built.\n","WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: one_step_text_gen_model/assets\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Assets written to: one_step_text_gen_model/assets\n"]}]},{"cell_type":"code","source":["states = None\n","# Pass in multiple inputs\n","next_char = tf.constant(['NEO:'])\n","result = [next_char]\n","\n","for n in range(100):\n","    next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n","    result.append(next_char)\n","\n","result = tf.strings.join(result)\n","# Just print out result for ease\n","print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HH6rQ7ekG-tS","executionInfo":{"status":"ok","timestamp":1643447746224,"user_tz":-480,"elapsed":2076,"user":{"displayName":"Adam Murphy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj859UcD_Pcry8yTAl_WjJig-Z9IB-ycFwNCz0WLFY=s64","userId":"08787610892899884155"}},"outputId":"ea4fbfae-6fc9-40fe-a244-5635a640c79a"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["NEO:\n","Brother, give the cause, then anon day's maid?\n","And sometials laughing me; it is as business\n","gone ac \n","\n","________________________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["## Customized Training\n","\n","The above technique uses *techer forcing* and prevents a bad prediction from being fed back into the model. But now we will let the model learn from its mistakes. \n","\n","This is an example of *curriculum learning*.\n","\n","[Customize what happens in Model.fit](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit) official TF tutorial."],"metadata":{"id":"eK0--aMZHq8e"}},{"cell_type":"code","source":["class CustomTraining(MyModel):\n","    @tf.function\n","    def train_step(self, inputs):\n","        inputs, labels = inputs\n","        with tf.GradientTape() as tape:\n","            predictions = self(inputs, training=True)\n","            loss = self.loss(labels, predictions)\n","        grads = tape.gradient(loss, self.trainable_variables)\n","        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n","        self.compiled_metrics.update_state(labels, predictions)\n","\n","        return {'loss': loss}"],"metadata":{"id":"yeHe-OpZI1JD","executionInfo":{"status":"ok","timestamp":1643448179215,"user_tz":-480,"elapsed":3,"user":{"displayName":"Adam Murphy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj859UcD_Pcry8yTAl_WjJig-Z9IB-ycFwNCz0WLFY=s64","userId":"08787610892899884155"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["model = CustomTraining(\n","    vocab_size=len(ids_from_chars.get_vocabulary()),\n","    embedding_dim=embedding_dim,\n","    rnn_units=rnn_units\n",")\n","model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n","\n","model.fit(dataset, epochs=2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uRFIdTVvI5Db","executionInfo":{"status":"ok","timestamp":1643448597841,"user_tz":-480,"elapsed":71448,"user":{"displayName":"Adam Murphy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj859UcD_Pcry8yTAl_WjJig-Z9IB-ycFwNCz0WLFY=s64","userId":"08787610892899884155"}},"outputId":"d2016da3-73ed-453b-a570-266cf51299c0"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","172/172 [==============================] - 30s 131ms/step - loss: 2.6923\n","Epoch 2/2\n","172/172 [==============================] - 26s 131ms/step - loss: 1.9695\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fd795593f10>"]},"metadata":{},"execution_count":41}]},{"cell_type":"markdown","source":["Finally, let's look at a full blown custom training loop (just like you would have to write in PyTorch from scratch)."],"metadata":{"id":"yXBQbyoyKgkj"}},{"cell_type":"code","source":["EPOCHS = 5\n","mean = tf.metrics.Mean()\n","\n","for epoch in range(EPOCHS):\n","    start = time.time()\n","\n","    mean.reset_states()\n","    for (batch_n, (inp, target)) in enumerate(dataset):\n","        logs = model.train_step([inp, target])\n","        mean.update_state(logs['loss'])\n","\n","        if batch_n % 50 == 0:\n","            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n","            print(template)\n","\n","    # Save model checkpoint every 5 epochs\n","    if (epoch + 1) % 5 == 0:\n","        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n","\n","    print()\n","    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n","    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n","    print(\"_\"*80)\n","    \n","# Save weights at end of training\n","model.save_weights(checkpoint_prefix.format(epoch=epoch))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QqSuNT6tKl4c","executionInfo":{"status":"ok","timestamp":1643448833021,"user_tz":-480,"elapsed":121710,"user":{"displayName":"Adam Murphy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj859UcD_Pcry8yTAl_WjJig-Z9IB-ycFwNCz0WLFY=s64","userId":"08787610892899884155"}},"outputId":"07864b2f-bdc1-4aa5-a23d-00db735d783b"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1 Batch 0 Loss 1.8444\n","Epoch 1 Batch 50 Loss 1.7242\n","Epoch 1 Batch 100 Loss 1.6455\n","Epoch 1 Batch 150 Loss 1.6016\n","\n","Epoch 1 Loss: 1.6965\n","Time taken for 1 epoch 26.23 sec\n","________________________________________________________________________________\n","Epoch 2 Batch 0 Loss 1.5920\n","Epoch 2 Batch 50 Loss 1.5355\n","Epoch 2 Batch 100 Loss 1.4879\n","Epoch 2 Batch 150 Loss 1.4967\n","\n","Epoch 2 Loss: 1.5406\n","Time taken for 1 epoch 22.91 sec\n","________________________________________________________________________________\n","Epoch 3 Batch 0 Loss 1.4967\n","Epoch 3 Batch 50 Loss 1.4656\n","Epoch 3 Batch 100 Loss 1.4318\n","Epoch 3 Batch 150 Loss 1.3821\n","\n","Epoch 3 Loss: 1.4440\n","Time taken for 1 epoch 23.84 sec\n","________________________________________________________________________________\n","Epoch 4 Batch 0 Loss 1.3807\n","Epoch 4 Batch 50 Loss 1.3958\n","Epoch 4 Batch 100 Loss 1.3817\n","Epoch 4 Batch 150 Loss 1.3342\n","\n","Epoch 4 Loss: 1.3780\n","Time taken for 1 epoch 23.29 sec\n","________________________________________________________________________________\n","Epoch 5 Batch 0 Loss 1.3391\n","Epoch 5 Batch 50 Loss 1.2821\n","Epoch 5 Batch 100 Loss 1.3663\n","Epoch 5 Batch 150 Loss 1.3343\n","\n","Epoch 5 Loss: 1.3255\n","Time taken for 1 epoch 23.50 sec\n","________________________________________________________________________________\n"]}]}],"metadata":{"interpreter":{"hash":"3ecf27939019150eaf754eaa3fa3d26a02c50bfda8c9a7333dada8ff5bafce6b"},"kernelspec":{"display_name":"Python 3.8.12 64-bit ('tensorflow': conda)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"orig_nbformat":4,"colab":{"name":"text_generation.ipynb","provenance":[],"collapsed_sections":["X37FYlI7mcg9","Mxj20hlVmcg9","IqbbO7NKmcg-"]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}