{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "First we will do training, then test. And we will process them differently. \n",
    "\n",
    "We MUST apply transformations to the training set, otherwise we will overfit massively. We need these spatial variations and modifications.\n",
    "\n",
    "We will do simple zooms and translations and rotations. This is Image Augmentation.\n",
    "\n",
    "`ImageDataGenerator` is great as it gives us batches of data in real time with the data aug we pass it. Awesome! Obvs this is a parameter that can and will need to be tuned from project to project. \n",
    "\n",
    "Lol this course instructor guy really does just focus on speed. Low quality but just enough that actually I am learning something useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gonna apply these transformations randomly to our images\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,      # automatically perform normalization!\n",
    "        shear_range=0.2,     # counter-clockwise rotation\n",
    "        zoom_range=0.2,      # random zoom amount\n",
    "        horizontal_flip=True # randomly flip horizontally\n",
    "        ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we *must* apply normalization and rescale our images. NNs are notorious for needing us to scale data before putting it in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Take path to a dir and generate batches of augmented data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'dataset/training_set',\n",
    "        target_size=(64, 64), # size of images when fed into CNN\n",
    "        batch_size=32,        # number of images per batch, 32 is classic\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note smaller target size will speed up training. Want to get it as small as possible while still getting good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Only apply normalization, don't apply data aug to test set\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Import test set images\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        'dataset/test_set',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now data processing is done! Time to build the CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, \n",
    "    MaxPool2D,\n",
    "    Flatten,\n",
    "    Dense\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Sequential([\n",
    "    Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=(64, 64, 3)),\n",
    "    MaxPool2D(pool_size=2, strides=2),\n",
    "    Conv2D(filters=32, kernel_size=3, activation='relu'),\n",
    "    MaxPool2D(pool_size=2, strides=2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `filters` is the number of feature detectors we want to the layer to have. Classic value to choose is 32 and then another 32. Has been shown to work well.\n",
    "* `kernel_size` is the size of the window e.g. 3x3. Can you pass a tuple (5, 4) to have non-square kernels. Same thing with `MaxPool2D` layers. \n",
    "* `strides` is the spaces to jump between kernel windows.\n",
    "* `input_shape` you should *always* specify the input shape of your images. I think this will be important for the TF exam, make sure you remember this. Since we manually resized the images in preprocessing and they are coloured, we know the answer is `(64, 64, 3)`. \n",
    "\n",
    "* Note for `padding` in `MaxPool2D` Hadelin checked it with using default and changing it and said it didn't change the results at all. Huh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='adam', \n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 82s 327ms/step - loss: 0.6989 - accuracy: 0.5322 - val_loss: 0.6383 - val_accuracy: 0.6225\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 95s 381ms/step - loss: 0.6279 - accuracy: 0.6549 - val_loss: 0.5995 - val_accuracy: 0.6640\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 96s 385ms/step - loss: 0.5742 - accuracy: 0.6996 - val_loss: 0.5588 - val_accuracy: 0.7140\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 93s 371ms/step - loss: 0.5314 - accuracy: 0.7347 - val_loss: 0.5271 - val_accuracy: 0.7455\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 100s 400ms/step - loss: 0.4936 - accuracy: 0.7606 - val_loss: 0.5041 - val_accuracy: 0.7640\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 95s 380ms/step - loss: 0.4856 - accuracy: 0.7681 - val_loss: 0.5516 - val_accuracy: 0.7395\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 93s 374ms/step - loss: 0.4753 - accuracy: 0.7722 - val_loss: 0.4937 - val_accuracy: 0.7605\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 93s 372ms/step - loss: 0.4508 - accuracy: 0.7833 - val_loss: 0.4696 - val_accuracy: 0.7905\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 82s 329ms/step - loss: 0.4362 - accuracy: 0.7972 - val_loss: 0.4510 - val_accuracy: 0.7985\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 81s 322ms/step - loss: 0.4254 - accuracy: 0.8028 - val_loss: 0.4692 - val_accuracy: 0.7775\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 76s 305ms/step - loss: 0.4210 - accuracy: 0.8085 - val_loss: 0.4520 - val_accuracy: 0.7935\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 71s 285ms/step - loss: 0.3976 - accuracy: 0.8157 - val_loss: 0.4544 - val_accuracy: 0.8020\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 71s 285ms/step - loss: 0.3926 - accuracy: 0.8143 - val_loss: 0.4891 - val_accuracy: 0.7895\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 73s 291ms/step - loss: 0.3715 - accuracy: 0.8301 - val_loss: 0.4671 - val_accuracy: 0.7960\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 66s 266ms/step - loss: 0.3602 - accuracy: 0.8344 - val_loss: 0.4503 - val_accuracy: 0.7985\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 75s 299ms/step - loss: 0.3633 - accuracy: 0.8403 - val_loss: 0.4447 - val_accuracy: 0.8000\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 68s 271ms/step - loss: 0.3392 - accuracy: 0.8472 - val_loss: 0.4892 - val_accuracy: 0.7945\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 62s 248ms/step - loss: 0.3361 - accuracy: 0.8550 - val_loss: 0.4899 - val_accuracy: 0.7845\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 0.3320 - accuracy: 0.8504 - val_loss: 0.4661 - val_accuracy: 0.8020\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 51s 205ms/step - loss: 0.3197 - accuracy: 0.8606 - val_loss: 0.4438 - val_accuracy: 0.8085\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 50s 199ms/step - loss: 0.2961 - accuracy: 0.8717 - val_loss: 0.5081 - val_accuracy: 0.7895\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 48s 194ms/step - loss: 0.3027 - accuracy: 0.8617 - val_loss: 0.4863 - val_accuracy: 0.7955\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 0.2795 - accuracy: 0.8789 - val_loss: 0.4651 - val_accuracy: 0.8070\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 50s 202ms/step - loss: 0.2721 - accuracy: 0.8849 - val_loss: 0.4729 - val_accuracy: 0.8070\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 49s 198ms/step - loss: 0.2672 - accuracy: 0.8823 - val_loss: 0.4901 - val_accuracy: 0.8115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x107ae43a0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x=train_generator,\n",
    "        validation_data=test_generator,\n",
    "        epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a Single Prediction\n",
    "\n",
    "It's nice to do this... I guess the point is that we need to know how to make single predictions in production?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads as PIL image format in size specified\n",
    "test_image = image.load_img(\n",
    "    'dataset/single_prediction/cat_or_dog_2.jpg',\n",
    "    target_size=(64, 64)\n",
    ")\n",
    "\n",
    "test_image = image.load_img(\n",
    "    'dataset/test_set/cats/cat.4607.jpg',\n",
    "    target_size=(64, 64)\n",
    ")\n",
    "\n",
    "# Now convert to an array - predict method expects 2D arrays as input\n",
    "test_image = image.img_to_array(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that everything in training is a batch, thus everything in predict must also be a batch even if it's just a batch of 1. So add the extra dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add batch dimension of 1\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "# Can also do test_image.resize(1, etc.) but the former means we can \n",
    "# forget about the other dim sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = cnn.predict(test_image)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if result[0][0] == 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'\n",
    "prediction"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3ecf27939019150eaf754eaa3fa3d26a02c50bfda8c9a7333dada8ff5bafce6b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('tensorflow': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
