{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('download/sunspots.csv', index_col=0)\n",
    "time_step = list(range(len(df)))\n",
    "sunspots = df['Monthly Mean Total Sunspot Number'].values\n",
    "\n",
    "series = np.array(sunspots)\n",
    "time = np.array(time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [[3 4 5 6]\n",
      " [2 3 4 5]]\n",
      "y =  [[7]\n",
      " [6]]\n",
      "x =  [[5 6 7 8]\n",
      " [4 5 6 7]]\n",
      "y =  [[9]\n",
      " [8]]\n",
      "x =  [[0 1 2 3]\n",
      " [1 2 3 4]]\n",
      "y =  [[4]\n",
      " [5]]\n"
     ]
    }
   ],
   "source": [
    "window_size = 5\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "dataset = (dataset\n",
    "    # Create windowed dataset\n",
    "    .window(window_size, shift=1, drop_remainder=True)\n",
    "    # Maps function and flattens result\n",
    "    .flat_map(lambda window: window.batch(window_size))\n",
    "    # Split into x, y\n",
    "    .map(lambda window: (window[:-1], window[-1:]))\n",
    "    # Shuffle 10 bec we have 10 items - not necessary for time series\n",
    "    .shuffle(buffer_size=10)\n",
    "    # Results in 3x batches of 2 items each\n",
    "    .batch(2)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "for x, y in dataset:\n",
    "    print('x = ', x.numpy())\n",
    "    print('y = ', y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "    dataset = (tf.data.Dataset.from_tensor_slices(series)\n",
    "        .window(window_size+1, shift=1, drop_remainder=True)\n",
    "        .flat_map(lambda window: window.batch(window_size+1))\n",
    "        # Does it matter if we shuffle before or after the map\n",
    "        .shuffle(shuffle_buffer)\n",
    "        .map(lambda window: (window[:-1], window[-1:]))\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_time = 3000\n",
    "time_train = time[:split_time]\n",
    "X_train = series[:split_time]\n",
    "\n",
    "time_val = time[split_time:]\n",
    "X_val = time[split_time:]\n",
    "\n",
    "\n",
    "window_size = 30\n",
    "batch_size = 32\n",
    "shuffle_buffer_size = 1000\n",
    "\n",
    "dataset = windowed_dataset(series, window_size, batch_size, shuffle_buffer_size)\n",
    "# Simple Linear Regression\n",
    "L0 = tf.keras.layers.Dense(1, input_shape=window_size)\n",
    "model = tf.keras.models.Sequential([L0])\n",
    "\n",
    "model.compile(\n",
    "    loss='mse',\n",
    "    optimizer=tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9)\n",
    ")\n",
    "model.fit(dataset, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 weights (since input size is 20 lag vars)\n",
    "# and one b intercept value\n",
    "print(f'Layer weights {L0.get_weights()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(series[1:21])\n",
    "model.predict(series[1:21][np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to forecast a time series\n",
    "forecast = []\n",
    "for time in range(len(series) - window_size):\n",
    "    pred = model.predict(series[time:time +  window_size][np.newaxis])\n",
    "    forecast.append(pred)\n",
    "# Split out for plotting\n",
    "forecast = [split_time - window_size:]\n",
    "results = np.array(forecast)[:, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.metrics.mean_absolute_error(X_val, results).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate scheduling and picking optimal LR\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
    "    lambda epoch: 1e-8 * 10**(epoch/20)\n",
    ")\n",
    "# Plot\n",
    "lrs = 1e-8 * (10 ** (np.arange(100)/20))\n",
    "# Just plot learning rates against the loss! Easy.\n",
    "plt.semilogx(lrs, history.history['loss'])\n",
    "plt.axis([1e-8, 1e-4, 0, 300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple RNNs and LSTMs will give better performance. Swap out `SimpleRNN` below for `LSTM` and/or `Bidirectional(LSTM)` for better performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    # Expand dims so input is 3D (and no need to modify input pipeline)\n",
    "    tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n",
    "                            input_shape=[None]), # model takes seqs of any length\n",
    "    tf.keras.layers.SimpleRNN(20, return_sequences=True),\n",
    "    tf.keras.layers.SimpleRNN(20),\n",
    "    tf.keras.layers.Dense(1),\n",
    "    # Data is in the 40-70 range but tanh outputs in [-1, 1]\n",
    "    # so scaling preds to the actual range can help\n",
    "    tf.keras.layers.Lambda(lambda x: x * 100.0)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `Conv1D` + `LSTM` layers will almost certainly result in best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, LSTM, Dense, Lambda, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_params = dict(\n",
    "    filters=32, \n",
    "    kernel_size=5,\n",
    "    strides=1, \n",
    "    padding='causal', # appropriate padding for temporal data\n",
    "    activation='relu',\n",
    "    input_shape=[None, 1],\n",
    ")\n",
    "# Note: for this to work, need to add the following to \n",
    "# 1st line of windowed_dataset func:\n",
    "# series = tf.expand_dims(series, axis=-1)\n",
    "model = Sequential([\n",
    "    Conv1D(**conv_params),\n",
    "    Bidirectional(LSTM(32, return_sequences=True)),\n",
    "    Bidirectional(LSTM(32)),\n",
    "    Dense(1),\n",
    "    Lambda(lambda x: x * 200)\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3ecf27939019150eaf754eaa3fa3d26a02c50bfda8c9a7333dada8ff5bafce6b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('tensorflow': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
